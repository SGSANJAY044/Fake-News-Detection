{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK5Cef-__h19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e7ad00-d839-4b27-a067-69667908392f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=f35f4bccd46f08e01d941f545eb605bc417dc6a543f2aa4b68dfcae8c407ecbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "from pyspark.ml.feature import IDF, Tokenizer, VectorAssembler\n",
        "from pyspark.ml.feature import StopWordsRemover, CountVectorizer\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.sql.functions import when, col, regexp_replace, concat, lit, length\n",
        "from pyspark.sql.types import FloatType, DoubleType\n",
        "from pyspark.ml.classification import NaiveBayesModel, NaiveBayes\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
      ],
      "metadata": {
        "id": "uBp8eJ5wn4fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Fake News Detection\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "mySchema = StructType([ StructField(\"index\", IntegerType(), True)\\\n",
        "                       ,StructField(\"title\", StringType(), True)\\\n",
        "                       ,StructField(\"author\", StringType(), True)\\\n",
        "                       ,StructField(\"text\", StringType(), True)\\\n",
        "                       ,StructField(\"label\", IntegerType(), True)])\n",
        "\n",
        "path = \"/content/train.csv\"\n",
        "pandas_df = pd.read_csv(path, sep=',', de)\n",
        "\n",
        "spark_df = spark.createDataFrame(pandas_df, schema = mySchema)\n"
      ],
      "metadata": {
        "id": "SJyOb7_9_uVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8a9cfc-aa8c-4fd9-be66-0a0911a2c74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-b0e5c5231753>:10: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
            "\n",
            "\n",
            "  pandas_df = pd.read_csv(path, sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False,engine=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McfxZd4Hki6j",
        "outputId": "948f3106-9eba-46fe-ca71-ddd6176cd2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+-----+\n",
            "|index|               title|              author|                text|label|\n",
            "+-----+--------------------+--------------------+--------------------+-----+\n",
            "|    0|House Dem Aide: W...|       Darrell Lucus|House Dem Aide: W...|    1|\n",
            "|    1|FLYNN: Hillary Cl...|     Daniel J. Flynn|Ever get the feel...|    0|\n",
            "|    2|Why the Truth Mig...|  Consortiumnews.com|Why the Truth Mig...|    1|\n",
            "|    3|15 Civilians Kill...|     Jessica Purkiss|Videos 15 Civilia...|    1|\n",
            "|    4|Iranian woman jai...|      Howard Portnoy|Print \\nAn Irania...|    1|\n",
            "|    5|Jackie Mason: Hol...|     Daniel Nussbaum|In these trying t...|    0|\n",
            "|    6|Life: Life Of Lux...|                 NaN|Ever wonder how B...|    1|\n",
            "|    7|Benoît Hamon Wins...|     Alissa J. Rubin|PARIS  —   France...|    0|\n",
            "|    8|Excerpts From a D...|                 NaN|Donald J. Trump i...|    0|\n",
            "|    9|A Back-Channel Pl...|Megan Twohey and ...|A week before Mic...|    0|\n",
            "|   10|Obama’s Organizin...|         Aaron Klein|Organizing for Ac...|    0|\n",
            "|   11|BBC Comedy Sketch...|     Chris Tomlinson|The BBC produced ...|    0|\n",
            "|   12|Russian Researche...|       Amando Flavio|The mystery surro...|    1|\n",
            "|   13|US Officials See ...|          Jason Ditz|Clinton Campaign ...|    1|\n",
            "|   14|Re: Yes, There Ar...|        AnotherAnnie|Yes, There Are Pa...|    1|\n",
            "|   15|In Major League S...|       Jack Williams|Guillermo Barros ...|    0|\n",
            "|   16|Wells Fargo Chief...|Michael Corkery a...|The scandal engul...|    0|\n",
            "|   17|Anonymous Donor P...|            Starkman|A Caddo Nation tr...|    1|\n",
            "|   18|FBI Closes In On ...|             The Doc|FBI Closes In On ...|    1|\n",
            "|   19|Chuck Todd: ’Buzz...|           Jeff Poor|Wednesday after  ...|    0|\n",
            "+-----+--------------------+--------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_rmv_nan_text = spark_df.filter(length(col(\"text\")) > 60)\n",
        "df_no_nan = (df_rmv_nan_text.withColumn(\"title\", when(col(\"title\") == \"NaN\", \" \").otherwise(col(\"title\"))))\n",
        "df_clean = (df_no_nan.withColumn(\"title\", regexp_replace(col('title'),r'[^\\w\\’ ]','')).withColumn(\"text\", regexp_replace(col('text'),r'[^\\w\\’ ]','')).withColumn(\"text\", regexp_replace(col('text'),r'[ ]{2,}',' ')).withColumn(\"title\", regexp_replace(col('text'),r'[ ]{2,}',' '))\n",
        "                )\n",
        "df_combined = (df_clean\n",
        "                    .withColumn('full_text',\n",
        "                                  when(col(\"text\").contains(\n",
        "                                                    concat(col(\"title\"))),\n",
        "                                                    col(\"text\"))\n",
        "\n",
        "                                  .otherwise(concat(col(\"title\"),\n",
        "                                                    lit(\" \"),\n",
        "                                                    col(\"text\"))))\n",
        "                    .select([\"full_text\",\"label\"])\n",
        "                    .withColumn(\"label\", col(\"label\").cast(DoubleType()))\n",
        "                    .dropDuplicates()\n",
        "                )\n",
        "\n",
        "\n",
        "del df_rmv_nan_text, df_no_nan, df_clean\n",
        "\n",
        "print(df_combined.count())\n",
        "df_combined.show(7)"
      ],
      "metadata": {
        "id": "AyUnPTcsTdk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a487787-8c19-4574-d3b2-682fc4d1dd0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20210\n",
            "+--------------------+-----+\n",
            "|           full_text|label|\n",
            "+--------------------+-----+\n",
            "|0 0 AP N1 26 27 1...|  1.0|\n",
            "|GREENBELT Md The ...|  0.0|\n",
            "|The Minnesota off...|  0.0|\n",
            "|GeoEngineering Un...|  1.0|\n",
            "|Following a fight...|  0.0|\n",
            "|The military indu...|  1.0|\n",
            "|Insists Russia De...|  1.0|\n",
            "+--------------------+-----+\n",
            "only showing top 7 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined.groupby(\"label\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZRUdo-tr9MG",
        "outputId": "a770645b-d0ab-488f-df8c-08c4a7d5936d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|  0.0|10385|\n",
            "|  1.0| 9825|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    stopwords_ls = stopwords.words('english')\n",
        "except:\n",
        "    nltk.download(\"stopwords\")\n",
        "    stopwords_ls = stopwords.words('english')\n",
        "\n",
        "stopwords_ls[:10]"
      ],
      "metadata": {
        "id": "tf7dYGkSo5IE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148b916d-ee3f-4c6f-fc6f-fd96a3bc050b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "\n",
        "\n",
        "class Stemmer(Transformer,\n",
        "                 HasInputCol,\n",
        "                 HasOutputCol,\n",
        "                 DefaultParamsReadable,\n",
        "                 DefaultParamsWritable):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol = \"input\", outputCol = \"output\"):\n",
        "        super(Stemmer, self).__init__()\n",
        "        kwargs = self._input_kwargs\n",
        "        self.set_params(**kwargs)\n",
        "\n",
        "    @keyword_only\n",
        "    def set_params(self, inputCol = \"input\", outputCol = \"output\"):\n",
        "        kwargs = self._input_kwargs\n",
        "        self._set(**kwargs)\n",
        "\n",
        "    def get_input_col(self):\n",
        "        return self.getOrDefault(self.inputCol)\n",
        "\n",
        "    def get_output_col(self):\n",
        "        return self.getOrDefault(self.outputCol)\n",
        "\n",
        "    def _transform(self, df):\n",
        "\n",
        "        input_col = self.get_input_col()\n",
        "        output_col = self.get_output_col()\n",
        "\n",
        "\n",
        "        ps = PorterStemmer()\n",
        "\n",
        "        transform_udf = F.udf(lambda x: [ps.stem(word) for word in x], ArrayType(StringType(), False))\n",
        "\n",
        "        return df.withColumn(output_col, transform_udf(input_col))"
      ],
      "metadata": {
        "id": "vJNIKC-uuSDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df_combined.randomSplit([0.7,0.3], seed=2)"
      ],
      "metadata": {
        "id": "-OcobknvOhr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer(inputCol=\"full_text\", outputCol=\"full_text_words\")\n",
        "\n",
        "word_remover = StopWordsRemover(stopWords = stopwords_ls,\n",
        "                                inputCol = \"full_text_words\",\n",
        "                                outputCol = \"full_text_words_clean\")\n",
        "\n",
        "stemmer = Stemmer(inputCol = \"full_text_words_clean\", outputCol = \"stemmed\")\n",
        "\n",
        "tf = CountVectorizer(inputCol=\"stemmed\", outputCol=\"features\", vocabSize = 1e6)\n",
        "\n",
        "pipeline = Pipeline(stages= [tokenizer, word_remover, stemmer, tf]).fit(train)\n",
        "train_df = pipeline.transform(train).select([\"full_text\",\"features\",\"label\"])\n",
        "test_df = pipeline.transform(test).select([\"full_text\",\"features\",\"label\"])"
      ],
      "metadata": {
        "id": "8j6adcN_2jxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\", thresholds = [0.6, 0.4])\n",
        "nb_model = nb.fit(train_df)\n",
        "predictions_nb = nb_model.transform(test_df)"
      ],
      "metadata": {
        "id": "iabHMfw_48gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(df, labelCol=\"label\", predCol=\"prediction\"):\n",
        "    labels = df[labelCol].tolist()\n",
        "    predictions = df[predCol].tolist()\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision = precision_score(labels, predictions)\n",
        "    recall = recall_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    confusion = confusion_matrix(labels, predictions)\n",
        "\n",
        "    print(\"Accuracy: %.3f\" % accuracy)\n",
        "    print(\"Recall: %.3f\" % recall)\n",
        "    print(\"Precision: %.3f\" % precision)\n",
        "    print(\"F1 Score: %.3f\" % f1)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion)\n",
        "\n",
        "    return confusion, precision, recall\n",
        "\n"
      ],
      "metadata": {
        "id": "ZReao9t9qI9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(predictions_nb.select([\"label\",\"prediction\"]).toPandas())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLJlkWQ4TrZY",
        "outputId": "4e3d4231-6c46-4451-b282-4af4c01097c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.921\n",
            "Recall: 0.868\n",
            "Precision: 0.968\n",
            "F1 Score: 0.915\n",
            "Confusion Matrix:\n",
            "[[2985   84]\n",
            " [ 387 2540]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[2985,   84],\n",
              "        [ 387, 2540]]),\n",
              " 0.9679878048780488,\n",
              " 0.867782712675094)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}